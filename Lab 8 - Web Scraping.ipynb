{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup Documentation\n",
    "    https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "    https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n",
    "\n",
    "### Useful Functions\n",
    "    find_all()  : looks through a tag's descendants and returns a list of all descendants matching the filter\n",
    "    find()      : returns the first descendant matching the filter\n",
    "    find_next() : returns the first 'sibling' matching the filter, that appears immediately after the current tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Structure for Scrape.py\n",
    "    (1) Define the starting point (the base webpage)\n",
    "    (2) Use requests.get(<URL of the starting base webpage>) : to get a HTML document\n",
    "    (3) Use BeautifulSoup(response.text, 'html parser') : to extract data out of the HTML document\n",
    "    (4) Use page.find_all('b')[-1] to get a list of all bold tags\n",
    "    (5) (For) Loop over the list of bold tags to follow each 'bold tage' (URL) and retrieve the data on it\n",
    "            - Use for b in bold_tags: b.find('a')\n",
    "            - Each iteration call a function 'get_data' to follow link and get data (should return a dictionary)\n",
    "                  - Check what a.attrs gives you\n",
    "                  - For each 'a' you will have to follow Steps (2) & (3) : get HTML document, use BS4 to extract data from it\n",
    "                  - headers = ['Date', 'Operator', 'Flight origin', 'Destination', 'Fatalities']\n",
    "                  - Insert the 'Name' of the accident in the dictionary to return under key = 'Name'\n",
    "                  - Iterating over the list above, call another function 'get_accident_data' \n",
    "                        - Use th = a_page.find('th', text=header) to find the table which contains the information\n",
    "                        - Use th.find_next('td') to get the data stored in the table\n",
    "            - append the result of 'get_data' function into a larger list (this will be a list of dictionaries)\n",
    "\n",
    "\n",
    "    (6) Finally, once the Loop over bold_tags ends, convert your list of dictionaries into a DataFrame\n",
    "    (7) Drop duplicates : Use df.drop_duplicates(inplace=True)\n",
    "    (8) Write the DataFrame onto a CSV file (without the Index) : Use df.to_csv('accidents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1\n",
    "base = 'https://en.wikipedia.org'\n",
    "path = '/wiki/List_of_accidents_and_incidents_involving_commercial_aircraft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2\n",
    "response = requests.get(base + path)\n",
    "#base + path\n",
    "#response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3\n",
    "page = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4\n",
    "bold_tags = page.find_all('b')[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands and Outputs below are rough work from the TA Session\n",
    "### Just for reference, use it carefully :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in bold_tags:\n",
    "    a = b.find('a')\n",
    "    get_accidents_data(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '/wiki/Southwest_Airlines_Flight_1380',\n",
       " 'title': 'Southwest Airlines Flight 1380'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if bold_tags[0].find('a') == None:\n",
    "    pass\n",
    "else:\n",
    "    <code>\n",
    "'''\n",
    "#request_2 = requests.get(base + a.attrs['href'])\n",
    "a.attrs\n",
    "b.find('a', href='/wiki/Southwest_Airlines_Flight_1380')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_2 = BeautifulSoup(request_2.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dallas Love Field,\\nDallas, Texas'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_2.find('th', text='Destination').find_next('td').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Southwest_Airlines_Flight_1380'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base + a.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Date', 'Operator', 'Flight origin', 'Destination', 'Fatalities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date : April 17, 2018\n",
      "Operator : Southwest Airlines\n",
      "Flight origin : LaGuardia Airport,\n",
      "New York City, New York\n",
      "Destination : Dallas Love Field,\n",
      "Dallas, Texas\n",
      "Fatalities : 1\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for header in headers:\n",
    "    get_accidents_data(a, header)\n",
    "    txt = page_2.find('th', text=header).find_next('td').text\n",
    "    print(header, ':', txt)\n",
    "    d[header] = txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'April 17, 2018',\n",
       " 'Destination': 'Dallas Love Field,\\nDallas, Texas',\n",
       " 'Fatalities': '1',\n",
       " 'Flight origin': 'LaGuardia Airport,\\nNew York City, New York',\n",
       " 'Name': 'Southwest Airlines Flight 1380',\n",
       " 'Operator': 'Southwest Airlines'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['Name'] = a.attrs['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(string):\n",
    "    <code>\n",
    "    return n\n",
    "\n",
    "df['Fatalities_Count'] = df['Fatalities'].str.apply(my_func, axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
